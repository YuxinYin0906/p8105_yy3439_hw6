---
title: "P8105 HW6"
output: github_document
date: "2023-12-02"
---
# Problem 2

```{r}
library(tidyverse)
library(ggplot2)
set.seed(1)

#download data
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

## Step 1: Fit the Regression Line

```{r}
fit = lm(tmax ~ tmin + prcp, data = weather_df)
```
## Step 2: Bootstrapping

Firstly, create a bootstrapping function.

```{r}
boot_sample = function(df){
  sample_frac(df, replace = TRUE)
}
```

Then, we create 5000 bootstrapping samples by using a list column. 

```{r}
boot_straps = 
  tibble(strap_number = 1:5000)|>
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(df = weather_df))
  )

boot_straps
```

### Step 2.1: 95% Confidence Interval for $\hat{r^{2}}$

In addition, for each bootstrap sample, produce estimates of the two desired quantities.

```{r}
bootstrap_results = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df) ),
    results = map(models, broom::glance)) |> 
  select(-strap_sample, -models) |> 
  unnest(results) 

# identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for estimated r^2
r_squared = bootstrap_results |> 
  summarize(
    lower_r_squared = quantile(r.squared, 0.025),
    upper_r_squared = quantile(r.squared, 0.975)) |> 
  knitr::kable(digits = 3)
```
Therefore, the 2.5% and 97.5% quantiles to provide a 95% confidence interval for estimated $\hat{r^{2}}$ is approximately 0.889 and 0.941. In another word, we're 95% confident that the true estimated r^2 lies between 0.889 and 0.941. 


### Step 2.2: Plot the Distribution of $\hat{r^{2}}$

```{r}
bootstrap_results|>
  ggplot(aes(x = r.squared)) +
  geom_density(fill = "blue") +
  theme_minimal() +
  labs(
    title = "Estimated R-Squared Distribution Plot",
    x = "estimated r-squared",
    y = "density"
  )
```

Based on the density plot of estimated r-squared, we can see that the distribution is slightly left-skewed and most of the data points gathered around 0.91 to 0.92. Based on the definition of r_squared, we can say that, for most of the bootstrapping samples, large percent of the variability in the outcome can be explained by the regression model. As a result, this indicates that our model is in goodness of fit if we assess solely based on the r-squared distribution. 


### Step 2.3: 95% CI for $\log(\hat{\beta_0}*\hat{\beta_1})$

```{r}
logresult = 
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df) ),
    log_results = map(models, broom::tidy)) |> 
  select(-strap_sample, -models) |> 
  unnest(log_results)|>
  filter(term == "tmin"|term == "prcp")|>
  select(strap_number:estimate)|>
  pivot_wider(names_from = term,
              values_from = estimate)|>
  mutate(log_estimates = log(tmin*prcp))|>
  na.omit()|>
   summarize(
    lower_log_estimates = quantile(log_estimates, 0.025),
    upper_log_estimates = quantile(log_estimates, 0.975)) |> 
  knitr::kable(digits = 3)
  
```


Therefore, the 95% CI of $\log(\hat{\beta_0}*\hat{\beta_1})$ is approximately (-8.982, -4.602). In another word, we're 95% confident that the true estimated $\log(\hat{\beta_0}*\hat{\beta_1})$ coefficient lies between -8.982 and -4.602. 


### Step 2.4: Plot the Distribution of $\log(\hat{\beta_0}*\hat{\beta_1})$

```{r}
  boot_straps |> 
  mutate(
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df) ),
    log_results = map(models, broom::tidy)) |> 
  select(-strap_sample, -models) |> 
  unnest(log_results)|>
  filter(term == "tmin"|term == "prcp")|>
  select(strap_number:estimate)|>
  pivot_wider(names_from = term,
              values_from = estimate)|>
  mutate(log_estimates = log(tmin*prcp))|>
  na.omit()|>
  ggplot(aes(x = log_estimates)) +
  geom_density(fill = "yellow") +
  theme_minimal() +
  labs(
    title = "Estimated log(tmin*prcp) Distribution Plot",
    x = "estimated log(tmin*prcp)",
    y = "density"
  )
```

Based on the density plot of estimated $\log(\hat{\beta_0}*\hat{\beta_1})$, we can see that the distribution is left-skewed and most of the data points gathered around -6, which corresponds to our previous 95% CI. In addition, we can see that after log transformation, the distribution is not normal.


# Problem 3

## Step 3.0: Data Cleaning
```{r}
# import data
library(dplyr)
library(modelr)
birthweight = read_csv("./data/birthweight.csv")

birthweight_clean = birthweight|>
  mutate(mrace = as.factor(mrace))|>
  mutate(malform = as.factor(malform))|>
  mutate(babysex = as.factor(babysex))

 birthweight_fit = birthweight_clean|>
   mutate(mrace = case_match(mrace,
                              "1" ~ "White",
                              "2" ~ "Black",
                              "3" ~ "Asian",
                              "4" ~ "Puerto Rican"
                          ))|>
   mutate(malform = case_match(malform,
                               "0" ~ "absent",
                               "1" ~ "present"))
```
## Step 3.1: Fit a Proposed Regression Line

Then, we can fit the regression model given predictors: `fincome`, `malform`, `mrace`, and `smoken`. I chose the four predictors since family financial income can have an influential effect on the environment of the whole pregnancy period as well as the nutrition the mother received. In addition, `malform` refers to malformations, which could result infants' birth weight. I also include the `mrace` and `smoken` factors to examine if mother's race and smoking cigarettes result in different birth weights. 

```{r}
# Data Cleaning

fit_0 = lm(bwt ~ fincome + malform + mrace + smoken,
           data = birthweight_fit)
```

## Step 3.2: Plot of Residuals Against Fitted Values

```{r}
#add residuals and predictions
birthweight_fit = birthweight_fit|>
  add_residuals(fit_0, var = "residuals")|>
  add_predictions(fit_0, var = "fitted")
```

Then, we can graph the plot of residuals against fitted values.

```{r}
ggplot(birthweight_fit, aes(x= fitted, y = residuals))+
  geom_point()+
  geom_hline(yintercept = 0, linetype = "dashed", color = "blue") +
  labs(title = "Residuals Against Fitted Values",
       x = "Fitted Values",
       y = "Residuals")
```

Based on the residual graph, we can conclude that roughly majority of the data points gathered around the line in which residual = 0. However, there are some eminent outliers existing so we should be considerate about the protential effects these outliers might introduce to our statistical inference. 

## Step 3.3: Cross Validation

Then, we can compare our model `fit_0` to the other two models `fit_1` and `fit_2`.
```{r}
# fit_1: One using length at birth and gestational age as predictors (main effects only)
fit_1 = lm(bwt ~ blength + gaweeks, data = birthweight)

#fit_2: One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
fit_2 = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex, data = birthweight_clean)
```



